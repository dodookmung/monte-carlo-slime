# 📜 몬테카를로 슬라임의 법칙 (Rules of Monte Carlo Slime)

이 문서는 강화학습(RL)의 기초인 **몬테카를로(Monte Carlo) 방법론**을 사용하여 학습하는 픽셀 슬라임의 시뮬레이션 규칙을 정의합니다.

## 1. 세계관 (Environment)
* **크기:** 5x5 격자 (Grid World). 좌표는 (0,0)에서 (4,4)까지 존재한다.
* **시작 지점 (Start):** (0, 0) - 좌측 상단.
* **목표 지점 (Goal):** (4, 4) - 우측 하단. 이곳에는 '사과'가 있다.
* **함정 (Trap):** (2, 2) - 중앙. 이곳에는 '구덩이'가 있다. (선택 사항)
* **상태 (State):** 슬라임이 현재 위치한 좌표 (x, y).

## 2. 슬라임 (Agent)
* **생김새:** 하나의 픽셀 (🟩).
* **행동 (Action):** 슬라임은 매 턴마다 상(Up), 하(Down), 좌(Left), 우(Right) 중 한 방향으로 움직일 수 있다.
* **제약:** 격자 밖(벽)으로는 이동할 수 없다. 벽에 부딪히면 제자리에 머문다.

## 3. 보상 체계 (Reward System)
슬라임이 행동을 할 때마다 다음과 같은 점수를 부여하여 행동을 강화하거나 억제한다.

1. **목표 도착 (Goal):** `+10점` (성공! 🥳) -> 에피소드 종료.
2. **함정 빠짐 (Trap):** `-10점` (실패! ☠️) -> 에피소드 종료.
3. **일반 이동 (Step):** `-1점`
    * *이유:* 슬라임이 게으름 피우지 않고 최단 경로로 움직이도록 유도하기 위해, 시간이 지날수록 점수를 깎는다.

## 4. 몬테카를로 법칙 (The Logic)
우리는 **"경험(Episode)이 끝난 뒤에 복기하는 방식"**을 사용한다.

1. **탐험 (Exploration):**
   - 슬라임은 처음에는 지도가 없으므로 100% 무작위(Random)로 움직인다.
   - 목표에 도착하거나 함정에 빠질 때까지의 모든 경로(Path)를 기록해둔다.
   
2. **평가 (Evaluation):**
   - 에피소드가 끝나면, 슬라임이 지나온 길을 역추적한다.
   - **수식 대체 논리:** "내가 (2,1) 좌표에서 오른쪽으로 갔을 때, 결국 사과를 먹어서 총점 +5점을 받았네?" 
   - 이 경험을 해당 좌표의 가치(Value)로 기록한다.

3. **업데이트 (Update - The Learning):**
   - 같은 좌표를 여러 번 방문했다면, 그동안 얻은 점수들의 **평균(Average)**을 낸다.
   - *예시:* (1,1) 위치의 가치 = (첫 번째 시도 점수 + 두 번째 시도 점수 ...) / 시도 횟수
   - 이 평균 점수가 높을수록 그곳은 "좋은 땅"이다.

4. **정책 발전 (Improvement):**
   - 충분한 경험이 쌓이면(예: 100번 시도 후), 슬라임은 더 이상 무작위로 걷지 않고, 주변 4칸 중 **평균 점수가 가장 높은 칸**으로 이동하려고 한다.

## 5. 시뮬레이션 목표
* 총 1,000번의 에피소드를 반복한다.
* 처음 100번은 멍청하게 헤매지만, 900번째 쯤에는 슬라임이 최단 경로로 사과를 찾아가는지 확인한다.